{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80: Homework 09\n",
    "\n",
    "### Due Date: Wednesday, March 12, 11:59PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the homework problems and provides code and markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding work will be developed in an accompanying `hw0X.py` file, that will be imported into the current notebook. (`X` is a homework number)\n",
    "\n",
    "Homeworks and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying python file will be tested (a la DSC 20),\n",
    "2. The notebook will be graded (for graphs and free response questions).\n",
    "\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name. The dictionary at the end of the file (`GRADED FUNCTIONS`) contains the \"grading list\". The final function in the file allows your doctests to check that all the necessary functions exist.\n",
    "- If you changed something you weren't supposed to, just use git to revert!\n",
    "\n",
    "**Tips for working in the Notebook**:\n",
    "- The notebooks serve to present you the questions and give you a place to present your results for later review.\n",
    "- The notebook on *HW assignments* are not graded (only the `.py` file).\n",
    "- Notebooks for PAs will serve as a final report for the assignment, and contain conclusions and answers to open ended questions that are graded.\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `.py` file.\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional functions to solve the HW! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `hw0X.py` (much like we do in the notebook).\n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hw09 as hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "In this question you are given a few word problems that ask you to work with difference measures: specificity, sensitivity, precision.\n",
    "\n",
    "* A new diagnostic test has 93% sensitivity and 95% specificity. You decided to try it on a group of 10,000 people. Half of them are known to have the disease and half of them do not have it. How many of the *known* positives would actually test positive?  How many of the *known* negatives would actually test negative?\n",
    "\n",
    "* A new screening test for some disease A has 95% sensitivity and 93% specificity. You plan to screen a population in which the prevalence of the disease (<a href=\"https://en.wikipedia.org/wiki/Prevalence\" >meaning of \"prevalence\" </a>) is 0.3%.  What proportion of positive identifications was actually correct?\n",
    "\n",
    "Write a function `question1` that returns a list of you answers, in order. For example, the first 2 numbers are the answers to the first question, third number is the answer to the second question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mushroom classification\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "\"Although this dataset was originally contributed to the UCI Machine Learning repository nearly 30 years ago, mushroom hunting (otherwise known as \"shrooming\") is enjoying new peaks in popularity. Learn which features spell certain death and which are most palatable in this dataset of mushroom characteristics. And how certain can your model be?\"\n",
    "\n",
    "Citation: https://www.kaggle.com/uciml/mushroom-classification/version/1#_=_\n",
    "\n",
    "We will load the dataset using the pandas library. We see that each feature (column) in this dataset is comprised of a set of categories. The number of categories varies per feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/mushrooms.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine-learning algorithms cannot handle categorical features. In order to extend the possible algorithms, we can encode the features into a more permissive representation (use One-Hot encoding)\n",
    "\n",
    "* Split this dataset into labels (y) and features (X**). Reserve 1/3 of your data for testing.\n",
    "* Use three classification algorithms:\n",
    "    1. KNN classifier, with 1 neighbor\n",
    "    2. Bayesian classifier that assumes every feature is independent of every other feature (GaussianNB())\n",
    "    3. Random Forest Classifier with a single estimator, max depth 3, minimum samples split is 20, min samples leaf is 10. \n",
    "    \n",
    "* Test your models by comparing their F1 - scores (for label 'e').\n",
    "* Write a function `order_classifiers` that returns a list of three classifiers mentioned above sorted by the best F1 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faulty Scooters\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "A new electric scooter company 'Maxwell Scooters' opened a retail shop in La Jolla recently and 300 UCSD students bought new scooters for getting around campus. After 8 students start complaining their scooters are faulty, negative on-line reviews for the scooters start to spread. In response, the scooter company adamantly claims that 99% of their scooters come off the production line working properly. You think this seems unlikely and decide to investigate.\n",
    "\n",
    "* Select a significance level for you investigation. (Not to be turned in)\n",
    "* What are reasonable choices for the *Null Hypothesis* for your investigation? Select all that apply:\n",
    "    1. The scooter company produces scooters that are 99% non-faulty.\n",
    "    2. The scooter company produces scooters that are less than 99% non-faulty.\n",
    "    3. The scooter company produces scooters that are at least 1% faulty.\n",
    "    4. The scooter company produces scooters that are ~2.6% faulty.\n",
    "\n",
    "Return your answer in a function `null_hypoth` of zero variables.\n",
    "\n",
    "* Create a function `simulate_null` simulates a single step of data generation under the null hypothesis. The function should return a binary array.\n",
    "\n",
    "* Create a function `estimate_p_val` that takes in a number `N` and returns the estimated p-value of your investigation upon simulating the null hypothesis `N` times.\n",
    "\n",
    "*Note*: Plot the Null distribution and your observed statistic to check your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference in the police stops data\n",
    "\n",
    "These questions will pursue a few basic inference questions with the cleaned vehicle stops data from project 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_id</th>\n",
       "      <th>stop_cause</th>\n",
       "      <th>service_area</th>\n",
       "      <th>subject_race</th>\n",
       "      <th>subject_sex</th>\n",
       "      <th>subject_age</th>\n",
       "      <th>sd_resident</th>\n",
       "      <th>searched</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Equipment Violation</td>\n",
       "      <td>530.0</td>\n",
       "      <td>W</td>\n",
       "      <td>M</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Moving Violation</td>\n",
       "      <td>520.0</td>\n",
       "      <td>B</td>\n",
       "      <td>M</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Moving Violation</td>\n",
       "      <td>110.0</td>\n",
       "      <td>H</td>\n",
       "      <td>F</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Moving Violation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>F</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Moving Violation</td>\n",
       "      <td>230.0</td>\n",
       "      <td>W</td>\n",
       "      <td>M</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stop_id           stop_cause  service_area subject_race subject_sex  \\\n",
       "0        0  Equipment Violation         530.0            W           M   \n",
       "1        1     Moving Violation         520.0            B           M   \n",
       "2        2     Moving Violation         110.0            H           F   \n",
       "3        3     Moving Violation           NaN            W           F   \n",
       "4        4     Moving Violation         230.0            W           M   \n",
       "\n",
       "   subject_age  sd_resident  searched  dayofweek  hour  \n",
       "0         28.0          1.0       0.0          4     0  \n",
       "1         25.0          0.0       0.0          4     0  \n",
       "2         31.0          NaN       NaN          4     0  \n",
       "3         29.0          0.0       0.0          4     0  \n",
       "4         52.0          0.0       0.0          4     0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops = pd.read_csv('data/vehicle_stops_datasd.csv')\n",
    "stops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "Suppose you would like to answer the question: \"Does the likelihood that a traffic stop results in a search depend on location? Or are police equally likely to search any car that's pulled over?\"\n",
    "\n",
    "To investigate this question, perform a hypothesis test with significance level 0.01, using the null hypothesis: \"Any given stop is equally likely to result in a search, regardless of the `service_area` in which the stop occurred.\" Ignore missing values of `service_area` in this analysis.\n",
    "\n",
    "Measure the difference between the distribution of search under the null hypothesis and the observed distribution of searches using the total-variation distance.\n",
    "\n",
    "* Create a function `simulate_searches` that takes in the stops data and returns a function of zero variables that simulates a single step of data generation under the null hypothesis. The function should return a (hypothetical) empirical distribution of searches by service area. That is, if `sim = simulate_searches(stops)`, then calling `sim()` generates a distribution of searches.\n",
    "\n",
    "* Create a function `tvd_sampling_distr` that takes in the `stops` data and a number N and returns the distribution of tvds generated under the null-hypothesis. That is, your function should return an array of `N` floats.\n",
    "\n",
    "* Create a function `search_results` of zero variables that returns a tuple with the following information:\n",
    "    1. The value of the observed statistic.\n",
    "    2. `True` if you reject the null hypothesis, and `False` if you fail to reject the null hypothesis.\n",
    "    \n",
    "The values in `search_results` should be hard-coded.\n",
    "\n",
    "*Note:* [This chapter from DSC10](https://www.inferentialthinking.com/chapters/11/2/Multiple_Categories.html) should help guide you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "\n",
    "# obs_tvd, _ = hw.search_results()\n",
    "# pd.Series(hw.tvd_sampling_distr(stops, 1000)).plot(kind='hist')\n",
    "# plt.plot([obs_tvd,obs_tvd], [0,300], markersize=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null values: testing for MAR vs MCAR\n",
    "\n",
    "**Question 5**\n",
    "\n",
    "In this question, you will test for the missingness type of `sd_resident`. You will conclude that the column is missing dependent on `service_area`. \n",
    "\n",
    "Recall, the attribute `sd_resident` is *missing completely at random* if for *every* other attribute `col`, the following two distributions are \"the same\":\n",
    "* the distribution of `col` when `sd_resident` is present, and\n",
    "* the distribution of `col` when `sd_resident` is missing.\n",
    "\n",
    "Determining if two observed distributions come from the same process is exactly what AB-testing does. Thus, to determine if `sd_resident` is MCAR, we need to do a permutation test between these two distributions for *every (other) column* in the dataset.\n",
    "\n",
    "Perform a permutation test for the empirical distribution of `service_area` conditional on `sd_resident=NULL` with significance level 1%. As the column `service_area` is categorical, use the total variation distance as the measure between the two distributions.\n",
    "\n",
    "Create the following three functions:\n",
    "\n",
    "* A function `perm_test` that takes in the stops data and runs the above permutation procedure once. The function should return a float (the tvd between the two distributions).\n",
    "\n",
    "* A function `obs_stat` that takes in the stops data and computes the observed statistic of the permutation test.\n",
    "\n",
    "* A function `sd_res_missing_dependent` that takes in the stops data and a number `N`, and returns the p-value that tests whether `sd_resident` is missing at random dependent on `service_area`.\n",
    "\n",
    "* A function `sd_res_missing_cols` of zero variables that returns a list of columns for which the missingness of `sd_resident` is dependent on (use a 1% significance level). **Do not consider `stop_id` in your tests**.\n",
    "\n",
    "*Note:* Writing your function to work on any column (not just `service_area`) allows you to run this permutation test on *all columns* of the stops data -- which is what the last column is asking you to do! This allows you to determine *which* columns the missingness of `sd_resident` is dependent on -- to either determine it `sd_resident` is MCAR, or to determine how to impute the column.\n",
    "\n",
    "*Note:* Be sure to plot your sampling distributions and observed statistic to check your work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
